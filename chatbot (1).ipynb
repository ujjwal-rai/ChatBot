{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-NSxQdVXsJUF",
        "outputId": "18298ebd-176b-4169-8f2b-895aa67797cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3hosS-beZiPn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7OTLR3Hq4y2"
      },
      "source": [
        "Download necessary NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejCfjn8mYjJ",
        "outputId": "aed15857-98d2-40b9-a462-cd3389197eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xypCSvzq_kF"
      },
      "source": [
        "Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FTpW9zb6rBjY"
      },
      "outputs": [],
      "source": [
        "def load_json(file_path):\n",
        "    \"\"\"\n",
        "    Load a standard JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    \"\"\"Load a JSONL file and return a list of dictionaries.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return [json.loads(line) for line in file if line.strip()]  # Skip empty lines\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading JSONL file: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAiT24q5rFE9"
      },
      "source": [
        "Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CaKFon6arHOd"
      },
      "outputs": [],
      "source": [
        "# Initialize Lemmatizer and Stopwords\n",
        "lemmer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "remove_punct_dict = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by lowercasing, removing punctuation,\n",
        "    tokenizing, removing stopwords, and lemmatizing.\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(remove_punct_dict)\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRwacmxrLat"
      },
      "source": [
        "Load Corpus Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TxYhrHrerN-V"
      },
      "outputs": [],
      "source": [
        "# Define file\n",
        "speaker_file = 'speakers.json'\n",
        "index_file = 'index.json'\n",
        "conversations_file = 'conversations.json'\n",
        "utterances_file = 'utterances.jsonl'\n",
        "corpus_file = 'corpus.json'  # Optional, used if you prefer the aggregated corpus\n",
        "\n",
        "# Load JSON files\n",
        "try:\n",
        "    speakers = load_json(speaker_file)\n",
        "    index = load_json(index_file)\n",
        "    conversations = load_json(conversations_file)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error loading JSON file: {e}\")\n",
        "\n",
        "# Load JSONL file\n",
        "utterances = load_jsonl(utterances_file)\n",
        "\n",
        "# (Optional) Load corpus.json if you prefer using it\n",
        "# corpus = load_json(corpus_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbIgtwx4aAD",
        "outputId": "52c325a8-07af-40c3-bb7c-ff83638ce929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'L1045', 'conversation_id': 'L1044', 'text': 'They do not!', 'speaker': 'u0', 'meta': {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PRP', 'dep': 'nsubj', 'up': 1, 'dn': []}, {'tok': 'do', 'tag': 'VBP', 'dep': 'ROOT', 'dn': [0, 2, 3]}, {'tok': 'not', 'tag': 'RB', 'dep': 'neg', 'up': 1, 'dn': []}, {'tok': '!', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]}]}, 'reply-to': 'L1044', 'timestamp': None, 'vectors': []}]\n"
          ]
        }
      ],
      "source": [
        "print(utterances[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IosOHxDrUL9"
      },
      "source": [
        "Create Utterance Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3zORc4iUrXF-"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from utterance_id to utterance text\n",
        "utterance_dict = {utt['id']: utt['text'] for utt in utterances}\n",
        "\n",
        "# Create a list of all utterance texts\n",
        "all_utterances = [utt['text'] for utt in utterances]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm7pNKMwrZRu"
      },
      "source": [
        "Build Chatbot Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UMuv4zwrgAW",
        "outputId": "038e1645-90a7-4ecf-a380-2eeadda71e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing sentence embeddings...\n"
          ]
        }
      ],
      "source": [
        "# Initialize SentenceTransformer model for embeddings\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # You can choose other models if desired\n",
        "\n",
        "# Precompute sentence embeddings for all utterances\n",
        "print(\"Computing sentence embeddings...\")\n",
        "sentence_embeddings = model.encode(all_utterances, convert_to_tensor=True)\n",
        "\n",
        "# Greeting definitions\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"hey\", \"greetings\", \"sup\", \"what's up\", \"how are you\")\n",
        "GREETING_RESPONSES = [\"Hi there!\", \"Hello!\", \"Hey!\", \"Greetings!\", \"How can I assist you today?\"]\n",
        "\n",
        "# Fallback responses\n",
        "FALLBACK_RESPONSES = [\n",
        "    \"I'm not sure I understand. Could you elaborate?\",\n",
        "    \"That's interesting! Tell me more.\",\n",
        "    \"Can you clarify that?\",\n",
        "    \"I'm here to help! What do you mean?\"\n",
        "]\n",
        "\n",
        "def greet(sentence):\n",
        "    \"\"\"\n",
        "    Check if the user's input is a greeting and return a greeting response.\n",
        "    \"\"\"\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "    return None\n",
        "\n",
        "def fallback():\n",
        "    \"\"\"\n",
        "    Return a random fallback response when the bot doesn't understand.\n",
        "    \"\"\"\n",
        "    return random.choice(FALLBACK_RESPONSES)\n",
        "\n",
        "def get_response(user_input):\n",
        "    \"\"\"\n",
        "    Generate a response to the user's input using sentence embeddings and cosine similarity.\n",
        "    \"\"\"\n",
        "    # Preprocess and encode the user input\n",
        "    user_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarities between user input and all utterances\n",
        "    cosine_scores = cosine_similarity(\n",
        "        user_embedding.cpu().numpy().reshape(1, -1),\n",
        "        sentence_embeddings.cpu().numpy()\n",
        "    ).flatten()\n",
        "\n",
        "    # Find the index of the highest similarity score\n",
        "    top_idx = np.argmax(cosine_scores)\n",
        "    top_score = cosine_scores[top_idx]\n",
        "\n",
        "    # Define a similarity threshold\n",
        "    threshold = 0.5  # You can adjust this value based on testing\n",
        "\n",
        "    if top_score < threshold:\n",
        "        return fallback()\n",
        "    else:\n",
        "        return all_utterances[top_idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuUhSh7HrnHt"
      },
      "source": [
        "Chatbot Interaction Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ARKInY5ArpIl"
      },
      "outputs": [],
      "source": [
        "def chat():\n",
        "    \"\"\"\n",
        "    Start the chatbot interaction loop.\n",
        "    \"\"\"\n",
        "    print(\"Bot: Hello! I am the retrieval learning bot. Start typing your message after a greeting to talk to me. To end the conversation, type 'bye'!\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "        if user_input == 'bye':\n",
        "            print(\"Bot: Goodbye! Take care.\")\n",
        "            break\n",
        "        elif user_input in ['thanks', 'thank you']:\n",
        "            print(\"Bot: You're welcome!\")\n",
        "            continue\n",
        "\n",
        "        # Check for greeting\n",
        "        greeting_response = greet(user_input)\n",
        "        if greeting_response:\n",
        "            print(f\"Bot: {greeting_response}\")\n",
        "            continue\n",
        "\n",
        "        # Generate a response\n",
        "        response = get_response(user_input)\n",
        "        print(f\"Bot: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4lH4Ddyry91"
      },
      "source": [
        "Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDIdLYlYr33l",
        "outputId": "0fdc0095-28ee-4d77-dccf-73d05e9732ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hello! I am the retrieval learning bot. Start typing your message after a greeting to talk to me. To end the conversation, type 'bye'!\n",
            "You: hi\n",
            "Bot: Hi there!\n",
            "You: how are you\n",
            "Bot: How are you?\n",
            "You: i will kill you\n",
            "Bot: I'll kill you!\n",
            "You: shut up\n",
            "Bot: Shut up.\n",
            "You: why you are copying me\n",
            "Bot: Why are you following me?\n",
            "You: help me\n",
            "Bot: Help me.\n",
            "You: what is this behaviour?\n",
            "Bot: Why is this?\n",
            "You: you are dumb\n",
            "Bot: You're being stupid.\n",
            "You: bye\n",
            "Bot: Goodbye! Take care.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chat()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhDlz6i-cAem"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}